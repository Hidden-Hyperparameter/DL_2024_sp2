{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2dc0c4a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "91a04155783901da96bbe67b7a5e64bd",
     "grade": false,
     "grade_id": "cell-a336e6ffe0bd52b7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Deep Learning Coding Project 2: Image Classification\n",
    "\n",
    "Before we start, please put your **Chinese** name and student ID in following format:\n",
    "\n",
    "Name, 0000000000 // e.g.) 傅炜, 2021123123"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a648ddc",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "933b227c504b89bf0c43186e8d0e39a1",
     "grade": true,
     "grade_id": "cell-13ce984a5d4a067a",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "赵瀚宏, 2023040163"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7468705d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cdd86b19ae7e03e9618718ef3c6ea9a0",
     "grade": false,
     "grade_id": "cell-a68075035123f58c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "We will use Python 3, [NumPy](https://numpy.org/), and [PyTorch](https://pytorch.org/) for this coding project. The example code has been tested under the latest stable release version.\n",
    "\n",
    "### Task\n",
    "\n",
    "In this notebook, you need to train a model to classify images. Given an image, you need to distinguish its category,\n",
    "e.g., whether it is a horse or an automobile. There are total 10 classes:\n",
    "airplane, automobile, bird, cat, deer, dog, frog, horse, ship and truck. We\n",
    "release 40,000 images for training, 10,000 images for validation. Each image has\n",
    "a shape of (3, 128, 128). We will evaluate your model in 10,000 images on the test set.\n",
    "\n",
    "Download the dataset from [here](https://cloud.tsinghua.edu.cn/d/00e0704738e04d32978b/) and organize them into a folder named \"cifar_10_4x\".\n",
    "\n",
    "<!-- Images can be classified as \"No Finding\" or **one or more types**. In the basic task, given an image, you only need to tell whether the X-ray indicates \"Infiltration\". In the bonus task, you need to tell whether *each* of the diseases exists.\n",
    "\n",
    "Images are taken from the [ChestX-ray14 dataset](https://www.kaggle.com/nih-chest-xrays/data) and downsampled to (256, 256). We release 44872 gray scale images for training and validation. We will evaluate your model on 10285 images in the test set. The dataset is available [here](https://cloud.tsinghua.edu.cn/d/16d06a89c5b4459db703/) and organized as follows: `train` directory includes all images for training and validation, and each line of `train.txt` records the labels separated by \"|\". -->\n",
    "\n",
    "### Coding\n",
    "\n",
    "We provide a code template. You can add new cells and modify our example to train your own model. To run this code, you should:\n",
    "\n",
    "+ implement your model (named `Net`) in `model.py`.\n",
    "+ implement your training loop in this notebook\n",
    "\n",
    "Your final submitted model should not be larger than **20M**. **Using any pretrained model is NOT permitted**.\n",
    "Besides, before you submit your result, **make sure you can test your model using our evaluation cell.** Name your best model \"cifar10_4x_best.pth\".\n",
    "\n",
    "### Report & Submission\n",
    "\n",
    "Your report should include:\n",
    "\n",
    "1. the details of your model\n",
    "2. all the hyper-parameters\n",
    "3. all the tricks or training techniques you use\n",
    "4. the training curve of your submitted model.\n",
    "\n",
    "Reporting additional ablation studies and how you improve your model are also encouraged.\n",
    "\n",
    "You should submit:\n",
    "\n",
    "+ all codes\n",
    "+ the model checkpoint (only \"cifar10_4x_best.pth\")\n",
    "+ your report (a separate \"pdf\")\n",
    "\n",
    "to web learning. We will use the evaluation code in this notebook to evaluate your model on the test set.\n",
    "\n",
    "### Grading\n",
    "\n",
    "We will grade this coding project based on the performance of your model (70%) and your report (30%). Regarding the evaluation metric of your model, assume your test accuracy is $X$, then your score is\n",
    "\n",
    "$\\frac{min(X,H)−0.6}{H−0.6}×7$\n",
    "\n",
    "where $H$ is accuracy of the model trained by TAs and $H=0.9$, i.e., you will get the full score if your test accuracy is above 90%.\n",
    "\n",
    "**Bonus**: The best submission with the highest testing accuracy will get 1 bonus point for the final course grade.\n",
    "\n",
    "**Avoid plagiarism! Any student who violates academic integrity will be seriously dealt with and receive an F for the course.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f6d8c7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1ec5655a0d789db88709122c2a0ce6c9",
     "grade": false,
     "grade_id": "cell-4cee29f989d84cdc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Code Template\n",
    "\n",
    "We have masked the the training loop in this notebook for you to complete. You should also overwrite \"model.py\" and implement your own model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8c2354b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b34bc8b23f9c8e480a9671ef3453e7ac",
     "grade": false,
     "grade_id": "cell-a551fcc5ff27fb87",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b4fcaa",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4367d7a728b91da35685f558363c4d66",
     "grade": false,
     "grade_id": "cell-ce69007d45b9103b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Setup Code\n",
    "\n",
    "If you use Colab in this coding project, please uncomment the code, fill the `GOOGLE_DRIVE_PATH_AFTER_MYDRIVE` and run the following cells to mount your Google drive. Then, the notebook can find the required file. If you run the notebook locally, you can skip the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "785a7720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca391e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# # TODO: Fill in the Google Drive path where you uploaded the assignment\n",
    "# # Example: If you create a 2022SP folder and put all the files under CP1 folder, then '2022SP/CP1'\n",
    "# # GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = '2022SP/CP1'\n",
    "# GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = None \n",
    "# GOOGLE_DRIVE_PATH = os.path.join('drive', 'MyDrive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n",
    "# print(os.listdir(GOOGLE_DRIVE_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c62c2445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append(GOOGLE_DRIVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a227e03",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8b7e3bc021f202881dd19297b1144711",
     "grade": false,
     "grade_id": "cell-e11eaf041d72deda",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from dataset import CIFAR10_4x\n",
    "from evaluation import evaluation\n",
    "\n",
    "from model_dropout import Net  # this should be implemented by yourself"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01370c88",
   "metadata": {},
   "source": [
    "### Enjoy Your Coding Time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ca3c36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    seed = int(seed)\n",
    "    if seed < 0 or seed > (2**32 - 1):\n",
    "        raise ValueError(\"Seed must be between 0 and 2**32 - 1\")\n",
    "    else:\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "set_seed(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed366135",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root_dir = '.'\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([125 / 255, 124 / 255, 115 / 255],\n",
    "                         [60 / 255, 59 / 255, 64 / 255])\n",
    "    \n",
    "])\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(128,padding=4),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([125 / 255, 124 / 255, 115 / 255],\n",
    "                         [60 / 255, 59 / 255, 64 / 255])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "228d0018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-run for new dataset\n",
    "\n",
    "NUMOFWORKERS = 2\n",
    "trainset = CIFAR10_4x(root=data_root_dir,\n",
    "                      split=\"train\", transform=train_transform)\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=32, shuffle=True, num_workers=NUMOFWORKERS, pin_memory=True)\n",
    "\n",
    "validset = CIFAR10_4x(root=data_root_dir,\n",
    "                      split='valid', transform=transform)\n",
    "validloader = torch.utils.data.DataLoader(\n",
    "    validset, batch_size=32, shuffle=False, num_workers=NUMOFWORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc63d0c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of trained parameters: 2892170\n",
      "number of total parameters: 2892170\n"
     ]
    }
   ],
   "source": [
    "####################################\n",
    "#                                  #\n",
    "#                                  #\n",
    "#                                  #\n",
    "#                                  #\n",
    "#        RUN THIS FOR NEW MODEL    #\n",
    "#                                  #\n",
    "#                                  #\n",
    "#                                  #\n",
    "#                                  #\n",
    "####################################\n",
    "net = Net()\n",
    "print(\"number of trained parameters: %d\" % (\n",
    "    sum([param.nelement() for param in net.parameters() if param.requires_grad])))\n",
    "print(\"number of total parameters: %d\" %\n",
    "      (sum([param.nelement() for param in net.parameters()])))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "train_history = []\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
    "\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9e88bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12M\t./cifar10_4x_0.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NO EXECUTION\n",
    "model_dir = '.'\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "torch.save(net, os.path.join(model_dir, 'cifar10_4x_0.pth'))\n",
    "\n",
    "# check the model size\n",
    "os.system(' '.join(['du', '-h', os.path.join(model_dir, 'cifar10_4x_0.pth')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45e11f0a",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b9c6c195145fd44d054849497f0be5e3",
     "grade": false,
     "grade_id": "cell-2ea063d5855124d3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def train(epochs:int):\n",
    "    ##############################################################################\n",
    "    #                  TODO: You need to complete the code here                  #\n",
    "    ##############################################################################\n",
    "    # YOUR CODE HERE\n",
    "    # print(optimizer.param_groups[0])\n",
    "    trainset = CIFAR10_4x(root=data_root_dir,\n",
    "                      split=\"train\", transform=train_transform)\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=32, shuffle=True, num_workers=NUMOFWORKERS, pin_memory=True)\n",
    "    results = []\n",
    "    from tqdm import tqdm\n",
    "    from evaluation import evaluation\n",
    "    for epoch in range(epochs):\n",
    "        net.train()\n",
    "        with tqdm(trainloader) as bar:\n",
    "            nums = trues = 0\n",
    "            losses = 0\n",
    "            for i,(xb,yb) in enumerate(bar):\n",
    "                xb,yb = xb.to(device),yb.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                preds = net(xb)\n",
    "                with torch.no_grad():\n",
    "                    _, answer = torch.max(preds, 1)\n",
    "                    trues += (answer==yb).sum().item()\n",
    "                    nums += len(yb)\n",
    "                loss = criterion(preds,yb)\n",
    "                losses += loss\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                if(i%100==0):\n",
    "                    train_loss = losses/nums\n",
    "                    bar.set_description(f'Epoch {epoch}/{epochs}, accuracy {100*trues/nums}, train loss {train_loss}')\n",
    "        train_acc = 100*trues/nums\n",
    "        print(f'train accuracy: {train_acc}%')\n",
    "        net.eval()\n",
    "        nums = loss = 0\n",
    "        with tqdm(validloader) as bar:\n",
    "            with torch.no_grad():\n",
    "                for i,(xb,yb) in enumerate(bar):\n",
    "                    xb,yb = xb.to(device),yb.to(device)\n",
    "                    preds = net(xb)\n",
    "                    loss += criterion(preds,yb)\n",
    "                    nums += len(yb)\n",
    "        valid_loss = loss/nums\n",
    "        print(f'valid loss: {valid_loss}')\n",
    "        valid_acc =evaluation(net,validloader,device)\n",
    "        print(f'valid accuracy: {valid_acc}%')\n",
    "        results.append({'train accuracy':train_acc,'valid accuracy':valid_acc,'loss':(losses/nums).sum().item(),'valid loss':(valid_loss).sum().item()})\n",
    "    train_history.append({\n",
    "        'optimizer':{k:v for k,v in optimizer.param_groups[0].items() if k!='params'},\n",
    "        'epochs':epochs,\n",
    "        'results':results\n",
    "    })\n",
    "    return train_acc,valid_acc\n",
    "    # raise NotImplementedError()\n",
    "    ##############################################################################\n",
    "    #                              END OF YOUR CODE                              #\n",
    "    ##############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5cf717e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.enabled = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a37abfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87.59\n"
     ]
    }
   ],
   "source": [
    "net = torch.load('./models/cifar10_4x_03181902_acc90.51.pth')\n",
    "import json\n",
    "past = json.load(open('./models/cifar10_4x_03181902_acc90.51.json','r'))\n",
    "train_history = past['train_history']\n",
    "print(past['test_acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31af50b0",
   "metadata": {},
   "source": [
    "Note: 03181521\n",
    "re-train model of 0181515, if fail, then re-load model of 0181520."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e073ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0/10, accuracy 89.61542464612823, train loss 0.009557975456118584: 100%|██████████| 1250/1250 [00:23<00:00, 53.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 89.5625%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:02<00:00, 114.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss: 0.00940399058163166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the valid images: 90 %\n",
      "valid accuracy: 90.06%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, accuracy 89.77935054121565, train loss 0.009510748088359833: 100%|██████████| 1250/1250 [00:25<00:00, 49.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 89.775%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:02<00:00, 113.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss: 0.009752980433404446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the valid images: 89 %\n",
      "valid accuracy: 89.61%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, accuracy 89.91205245628643, train loss 0.009332055225968361: 100%|██████████| 1250/1250 [00:24<00:00, 50.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 89.855%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:02<00:00, 109.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss: 0.010283846408128738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the valid images: 89 %\n",
      "valid accuracy: 89.11%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, accuracy 89.56078268109908, train loss 0.009583720937371254: 100%|██████████| 1250/1250 [00:24<00:00, 51.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 89.54%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:02<00:00, 111.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss: 0.00923862960189581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the valid images: 90 %\n",
      "valid accuracy: 90.42%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, accuracy 89.90424646128227, train loss 0.00933544710278511: 100%|██████████| 1250/1250 [00:24<00:00, 50.07it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 89.8925%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:02<00:00, 112.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss: 0.009437349624931812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the valid images: 89 %\n",
      "valid accuracy: 89.93%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, accuracy 89.67006661115737, train loss 0.009473021142184734: 100%|██████████| 1250/1250 [00:24<00:00, 50.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 89.675%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:02<00:00, 110.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss: 0.009820458479225636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the valid images: 89 %\n",
      "valid accuracy: 89.81%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, accuracy 89.76894254787678, train loss 0.009405601769685745: 100%|██████████| 1250/1250 [00:24<00:00, 50.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 89.7625%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:02<00:00, 111.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss: 0.00976744294166565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the valid images: 89 %\n",
      "valid accuracy: 89.78%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, accuracy 89.75593255620316, train loss 0.009434766136109829: 100%|██████████| 1250/1250 [00:25<00:00, 49.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 89.745%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:02<00:00, 114.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss: 0.009515298530459404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the valid images: 89 %\n",
      "valid accuracy: 89.88%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, accuracy 89.86001248959201, train loss 0.009274689480662346: 100%|██████████| 1250/1250 [00:24<00:00, 51.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 89.845%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:02<00:00, 112.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss: 0.009617443196475506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the valid images: 90 %\n",
      "valid accuracy: 90.04%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, accuracy 89.8470024979184, train loss 0.009347698651254177: 100%|██████████| 1250/1250 [00:24<00:00, 51.63it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 89.85%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:02<00:00, 111.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss: 0.009676742367446423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the valid images: 89 %\n",
      "valid accuracy: 89.44%\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(net.parameters(),lr=3.8e-4,weight_decay=4.7e-4) \n",
    "tacc,vacc = train(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8cb145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for th in train_history[1:]:\n",
    "#     result = th['results']\n",
    "#     for res in result:\n",
    "#         try:\n",
    "#             del res['loss']\n",
    "#         except KeyError:\n",
    "#             pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae1cea6",
   "metadata": {},
   "source": [
    "# MANUAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d3817cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of model is 86.61999999999999 %\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "from test import fulltest\n",
    "import time\n",
    "\n",
    "DO_TEST = True\n",
    "test_acc = None\n",
    "current_time = time.strftime(\"%m%d%H%M\")\n",
    "model_dir = '.'\n",
    "try:\n",
    "    model_pth = os.path.join(model_dir,'models', f'cifar10_4x_{current_time}_acc{vacc}.pth')\n",
    "    torch.save(net, model_pth)\n",
    "except: \n",
    "    torch.save(net.state_dict(),os.path.join(model_dir,'models', f'cifar10_4x_{current_time}_acc{vacc}_dict.pt'))\n",
    "    print('only state dict saved!!')\n",
    "else: \n",
    "    if DO_TEST:\n",
    "        test_acc = fulltest(model_pth)\n",
    "model_src = ''\n",
    "with open('model.py','r') as f:\n",
    "    model_src=f.read()\n",
    "with open(os.path.join(model_dir,'models', f'cifar10_4x_{current_time}_acc{vacc}.json'),'w') as f:\n",
    "    import json\n",
    "    json.dump({\n",
    "        'model_type':model_src,\n",
    "        'train_history':train_history,\n",
    "        'description':\"\"\"\n",
    "Randomize the dataset after several train epochs, especially when the model seems to be stucked in some stage.\n",
    "                \"\"\",\n",
    "        'test_acc':None if test_acc is None else test_acc\n",
    "},f,indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f23347",
   "metadata": {},
   "source": [
    "# AUTOMIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee27791",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/queues.py\", line 244, in _feed\n",
      "    obj = _ForkingPickler.dumps(obj)\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/reduction.py\", line 51, in dumps\n",
      "    cls(buf, protocol).dump(obj)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/multiprocessing/reductions.py\", line 428, in reduce_storage\n",
      "    fd, size = storage._share_fd_cpu_()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/storage.py\", line 297, in wrapper\n",
      "    return fn(self, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/storage.py\", line 330, in _share_fd_cpu_\n",
      "    return super()._share_fd_cpu_(*args, **kwargs)\n",
      "RuntimeError: unable to write to file </torch_215180_3377634056_3>: No space left on device (28)\n",
      "ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).\n",
      "Epoch 0/10, accuracy 96.875, train loss 0.002522170078009367:   0%|          | 1/1250 [00:00<06:18,  3.30it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 215181) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1132\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1132\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/queue.py:180\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m--> 180\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 324\u001b[0m     gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/signal_handling.py:66\u001b[0m, in \u001b[0;36m_set_SIGCHLD_handler.<locals>.handler\u001b[0;34m(signum, frame)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhandler\u001b[39m(signum, frame):\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;66;03m# This following call uses `waitid` with WNOHANG from C side. Therefore,\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# Python can still get and update the process status successfully.\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m     \u001b[43m_error_if_any_worker_fails\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m previous_handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid 215181) is killed by signal: Bus error. It is possible that dataloader's workers are out of shared memory. Please try to raise your shared memory limit.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m cnt\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m99\u001b[39m:\n\u001b[1;32m      7\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(net\u001b[38;5;241m.\u001b[39mparameters(),lr\u001b[38;5;241m=\u001b[39mlr,weight_decay\u001b[38;5;241m=\u001b[39mweight_decay) \n\u001b[0;32m----> 8\u001b[0m     tacc,vacc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     test_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m vacc\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m90\u001b[39m:\n",
      "Cell \u001b[0;32mIn[17], line 15\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(epochs)\u001b[0m\n\u001b[1;32m     13\u001b[0m nums \u001b[38;5;241m=\u001b[39m trues \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     14\u001b[0m losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i,(xb,yb) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(bar):\n\u001b[1;32m     16\u001b[0m     xb,yb \u001b[38;5;241m=\u001b[39m xb\u001b[38;5;241m.\u001b[39mto(device),yb\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     17\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tqdm/std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1175\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1179\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1180\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1328\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1327\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1328\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1331\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1284\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1282\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m   1283\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[0;32m-> 1284\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1285\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1286\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1145\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1144\u001b[0m     pids_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(w\u001b[38;5;241m.\u001b[39mpid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[0;32m-> 1145\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpids_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) exited unexpectedly\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue\u001b[38;5;241m.\u001b[39mEmpty):\n\u001b[1;32m   1147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 215181) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "# automization\n",
    "# don't execute\n",
    "lr = 5e-4\n",
    "weight_decay = 5e-5\n",
    "cnt = 0\n",
    "while cnt<99:\n",
    "    optimizer = torch.optim.Adam(net.parameters(),lr=lr,weight_decay=weight_decay) \n",
    "    tacc,vacc = train(10)\n",
    "    test_acc = None\n",
    "    if vacc>=90:\n",
    "        test_acc = fulltest(model_pth)\n",
    "    import time\n",
    "    current_time = time.strftime(\"%m%d%H%M\")\n",
    "    try:\n",
    "        torch.save(net, os.path.join(model_dir,'models', f'cifar10_4x_{current_time}_acc{vacc}.pth'))\n",
    "    except:\n",
    "        torch.save(net.state_dict(),os.path.join(model_dir,'models', f'cifar10_4x_{current_time}_acc{vacc}_dict.pt'))\n",
    "    model_src = ''\n",
    "    with open('model.py','r') as f:\n",
    "        model_src=f.read()\n",
    "    with open(os.path.join(model_dir,'models', f'cifar10_4x_{current_time}_acc{vacc}.json'),'w') as f:\n",
    "        import json\n",
    "        json.dump({\n",
    "            'model_type':model_src,\n",
    "            'train_history':train_history,\n",
    "            'description':'Auto-run by machine',\n",
    "            'test_acc':test_acc\n",
    "    },f,indent=4)\n",
    "    if tacc-vacc > 1.2 and cnt>5:\n",
    "        if lr > 3e-5:\n",
    "            lr -= 0.5e-5\n",
    "        if weight_decay < 7e-4: \n",
    "            weight_decay += 1e-5\n",
    "    cnt+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf50ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# state_dicts = []\n",
    "# for _ in range(10):\n",
    "#     train(10)\n",
    "#     state_dicts.append(net.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e39ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the valid images: 78 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "78.22"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from evaluation import evaluation\n",
    "evaluation(net,validloader,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016783e9",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[81], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m model_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(model_dir,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcifar10_4x_best.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(net, model_path)\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      6\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(net, model_path)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# print(current_time)\n",
    "model_path = os.path.join(model_dir,'models', f'cifar10_4x_best.pth')\n",
    "torch.save(net, model_path)\n",
    "assert(False)\n",
    "try:\n",
    "    torch.save(net, model_path)\n",
    "except:\n",
    "    torch.save(net.state_dict(),model_path)\n",
    "    print('Saving has some errors')\n",
    "# torch.save(net, os.path.join(model_dir, f'cifar10_4x_{time.localtime()}.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e793356d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "885b0bf12e9c21f035c6df6199532ea9",
     "grade": false,
     "grade_id": "cell-a25d638df48b13bc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "Before submission, please run the following cell to make sure your model can be correctly graded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b631827",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "36e31059e96008e349a5ef3bccc487eb",
     "grade": false,
     "grade_id": "cell-3121e7d50ff7793b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of trained parameters: 2276170\n",
      "number of total parameters: 2276170\n",
      "can't load test set because [Errno 2] No such file or directory: '/root/DL/cifar_10_4x/test', load valid set now\n",
      "ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).\n",
      "\u0000ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).\n",
      "\u0000Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1132, in _try_get_data\n",
      "    data = self._data_queue.get(timeout=timeout)\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/queues.py\", line 113, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/connection.py\", line 424, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/connection.py\", line 931, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/opt/conda/lib/python3.10/selectors.py\", line 416, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/signal_handling.py\", line 66, in handler\n",
      "    _error_if_any_worker_fails()\n",
      "RuntimeError: DataLoader worker (pid 164178) is killed by signal: Bus error. It is possible that dataloader's workers are out of shared memory. Please try to raise your shared memory limit.\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/DL/evaluation.py\", line 45, in <module>\n",
      "    evaluation(net, testloader, device)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/root/DL/evaluation.py\", line 20, in evaluation\n",
      "    for data in dataLoader:\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 630, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1328, in _next_data\n",
      "    idx, data = self._get_data()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1294, in _get_data\n",
      "    success, data = self._try_get_data()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1145, in _try_get_data\n",
      "    raise RuntimeError(f'DataLoader worker (pid(s) {pids_str}) exited unexpectedly') from e\n",
      "RuntimeError: DataLoader worker (pid(s) 164178) exited unexpectedly\n"
     ]
    }
   ],
   "source": [
    "!python evaluation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258f97f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model save\n",
    "acc = 91\n",
    "import time\n",
    "current_time = time.strftime(\"%m%d%H%M\")\n",
    "os.rename(model_path,os.path.join(model_dir,'models', f'cifar10_4x_{current_time}_acc{acc}.pth'))\n",
    "model_src = ''\n",
    "\n",
    "with open('model.py','r') as f:\n",
    "    model_src=f.read()\n",
    "\n",
    "with open(os.path.join(model_dir,'models', f'cifar10_4x_{current_time}_acc{acc}.json'),'w') as f:\n",
    "    import json\n",
    "    json.dump({\n",
    "        'model_type':model_src,\n",
    "        'train_history':train_history,\n",
    "        'description':\"\"\"\n",
    "New net with smaller size\n",
    "    \"\"\"\n",
    "    },f,indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
